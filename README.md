# CBVideoSearch

pythonScripts 里面放着切分视频关键帧以及提取sift特征点的脚本，多进程并行化处理数据，不需要相互通信。
Kmeans 是本项目重点，用c语言实验了一个多线程并行化计算的聚类程序，用来建立词典。
CBVideoSearch 是搜索的，目前是暴力穷举

对MPI的定义是多种多样的，但不外乎下面三个方面，它们限定了MPI的内涵和外延：

MPI 是一个库，不是一门语言。MPI 提供库函数/过程供 C/C++/FORTRAN 调用。
MPI 是一种标准或规范的代表，而不特指某一个对它的具体实现。
MPI 是一种消息传递编程模型。最终目的是服务于进程间通信这一目标 。

进程(Process)

一个 MPI 并行程序由一组运行在相同或不同计算机 /计算节点上的进程或线程构成。为统一起见，我们将 MPI 程序中一个独立参与通信的个体称为一个进程。

进程组：

一个 MPI程序的全部进程集合的一个有序子集。进程组中每个进程都被赋予一个在改组中唯一的序号（rank），用于在该组中标识该进程。序号范围从 0 到进程数－1。

通信器（communicator）:

有时也译成通信子，是完成进程间通信的基本环境，它描述了一组可以互相通信的进程以及它们之间的联接关系等信息。MPI所有通信必须在某个通信器中进行。通信器分域内通信器（intracommunicator）和域间通信器（intercommunicator）两类，前者用于同一进程中进程间的通信，后者则用于分属不同进程的进程间的通信。

MPI 系统在一个 MPI 程序运行时会自动创建两个通信器：一个称为 MPI_COMM_WORLD，它包含 MPI 程序中所有进程，另一个称为MPI_COMM_SELF，它指单个进程自己所构成的通信器。

序号（rank）：

即进程的标识，是用来在一个进程组或一个通信器中标识一个进程。MPI 的进程由进程组/序号或通信器/序号唯一确定。

消息（message）：

MPI 程序中在进程间传递的数据。它由通信器、源地址、目的地址、消息标签和数据构成。

通信（communication）：

通信是指在进程之间进行消息的收发、同步等操作。



Kmeans
典型的算法如下，它是一种迭代的算法：

      （1）根据事先给定的k值建立初始划分，得到k个Cluster，比如，可以随机选择k个点作为k个Cluster的重心；

      （2）计算每个点到各个Cluster重心的距离，将它加入到最近的那个Cluster；

      （3）重新计算每个Cluster的重心；

      （4）重复过程2~3，直到各个Cluster重心在某个精度范围内不变化或者达到最大迭代次数。

局部性较好，容易并行化，对大规模数据集很有意义；算法时间复杂度是：O(nkt)，其中：n 是聚类点个数，k 是Cluster个数，t 是迭代次数。



Search

把图片构建成为 局部敏感哈希索引(LSH)
然后去检索








1、假设训练集有M幅图像，对训练图象集进行预处理。包括图像增强，分割，图像统一格式，统一规格等等。
2、提取SIFT特征。对每一幅图像提取SIFT特征（每一幅图像提取多少个SIFT特征不定）。每一个SIFT特征用一个128维的描述子矢量表示，假设M幅图像共提取出N个SIFT特征。
3、用K-means对2中提取的N个SIFT特征进行聚类，K-Means算法是一种基于样本间相似性度量的间接聚类方法，此算法以K为参数，把N个对象分为K个簇，以使簇内具有较高的相似度，而簇间相似度较低。聚类中心有k个（在BOW模型中聚类中心我们称它们为视觉词），码本的长度也就为k，计算每一幅图像的每一个SIFT特征到这k个视觉词的距离，并将其映射到距离最近的视觉词中（即将该视觉词的对应词频+1）。完成这一步后，每一幅图像就变成了一个与视觉词序列相对应的词频矢量。
4、构造码本。码本矢量归一化因为每一幅图像的SIFT特征个数不定，所以需要归一化。也就是说，将每张图片的特征个数变为频数，这样就可以防止因为提取特征数不同而造成的分类不准问题了。


bag of visual word 的模型建立过程：1、利用sift等特征提取算法从每张图片中提取特征点也就是视觉单词。在代码中，每张图片是200*200大小，然后步长是8，将图片分成16*16的小patch，这样就有576个小patch，在每个小patch上都进行sift提取关键点，每个小patch上有一个关键点，这样就有576个关键点，也就是每张图片最终变成了576个128维的向量（sift特征点是128维的），也就是576*128这样大小的一个矩阵。图片包括训练样本和测试样本一共有360张图片，所以数据一共就是360*576*128.    2、利用K-means进行聚类，构建词汇表vocabulary。在代码中是找了300个聚类中心，也就是300*128，聚类中心个数的选取从几百到上千不等，一般数据越大，聚类中心越多。聚类的数据是训练数据240*576*128和测试数据120*576*128，最大迭代次数100，聚类完成后得到300个聚类中心，每个是1*128维的向量。3、利用得到的聚类中心得到词汇表以后，对360张图片进行直方图统计，也就是看每张图片中的576个关键点与哪个聚类中心的距离最小（最相似），然后再最近的那个聚类中心所代表的1-300之间的数上加1，这样最终得到了BOW的数据300*360大小的矩阵，这里面300*240是训练数据，300*120是测试数据。注意由于这里每张图片的关键点的数目都是一样的，所以归一化的影响并不是特别的关键，但是如果每张图片上关键点的数目不是一样的，那就必须进行归一化，也就是将词数变成词频,就是除以总点数。
